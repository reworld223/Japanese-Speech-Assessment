{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8812e1ee-1b73-45c8-81d6-dd033a929b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d90f9d15a84c25932975d17e253f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63470c83ed174a3ab92f5d86be2c70e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/278 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1e3433807c4db5bc373d101373beb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74656ce3c5cd47a982baa5c6bc389903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"slplab/wav2vec2-xls-r-300m-japanese-hiragana\")\n",
    "\n",
    "vocab_size = processor.tokenizer.vocab_size\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebf4ee76-d4ec-446f-99b3-4041c91134a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import HubertForCTC\n",
    "\n",
    "model = HubertForCTC.from_pretrained('./hubert_ASR/checkpoint-17000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99dc6250-7647-4f35-9326-1dffecd71358",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HubertForCTC(\n",
       "  (hubert): HubertModel(\n",
       "    (feature_extractor): HubertFeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x HubertLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): HubertFeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): HubertEncoderStableLayerNorm(\n",
       "      (pos_conv_embed): HubertPositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): HubertSamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x HubertEncoderLayerStableLayerNorm(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=122, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f0d96e0-4ebb-4de6-90e9-4dd47d21dbde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.output_hidden_states=True\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(torch.rand(1, 16000).to(device))\n",
    "len(output.hidden_states)\n",
    "# 跟 HubertModel一樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b2f066-16a2-48c9-a0db-b20a02255502",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07019f837b0495b89be3ff95dc7d7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0ac710a856411696d457f2761799e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fda2e737e5494a82b6ae2f4a9c16d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/521k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9efd551260b435a89d0ea1b81f80fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed076c06ea2f42569a448131ccdb2518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['name', 'audio', 'transcription'],\n",
       "        num_rows: 5323\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"reazon-research/reazonspeech\", \"tiny\", trust_remote_code=True) # smail\n",
    "# ds in ~/.cache/huggingface/datasets \n",
    "# rm -rf ~/.cache/huggingface/\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b314cee9-5011-491f-94c8-ef4791a3be53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '000/5c02961e1ac71.flac',\n",
       " 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/aa46595380aebad1bf1a98a4aaaa63bbd328fec667ea8840e90dc8254550c614/000/5c02961e1ac71.flac',\n",
       "  'array': array([0.00735474, 0.00683594, 0.00674438, ..., 0.01919556, 0.01898193,\n",
       "         0.02380371]),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'あのときのこと覚えてる？'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets = ds[\"train\"].train_test_split(test_size=0.99)\n",
    "\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "test_dataset = split_datasets[\"test\"]\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "833d258e-a667-4f4e-8c8d-b730af700732",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_997/970523411.py:14: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode(\"J\",\"H\")\n",
      "/tmp/ipykernel_997/970523411.py:15: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode(\"K\",\"H\")\n",
      "/tmp/ipykernel_997/970523411.py:16: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode(\"r\",\"Hepburn\")\n",
      "/tmp/ipykernel_997/970523411.py:17: DeprecationWarning: Call to deprecated method getConverter. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  conv = kakasi.getConverter()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import MeCab\n",
    "import pykakasi\n",
    "\n",
    "CHARS_TO_IGNORE = [\",\", \"?\", \"¿\", \".\", \"!\", \"¡\", \";\", \"；\", \":\", '\"\"', \"%\", '\"', \"�\", \"ʿ\", \"·\", \"჻\", \"~\", \"՞\",\n",
    "          \"؟\", \"،\", \"।\", \"॥\", \"«\", \"»\", \"„\", \"“\", \"”\", \"「\", \"」\", \"‘\", \"’\", \"《\", \"》\", \"(\", \")\", \"[\", \"]\",\n",
    "          \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"…\", \"–\", \"°\", \"´\", \"ʾ\", \"‹\", \"›\", \"©\", \"®\", \"—\", \"→\", \"。\",\n",
    "          \"、\", \"﹂\", \"﹁\", \"‧\", \"～\", \"﹏\", \"，\", \"｛\", \"｝\", \"（\", \"）\", \"［\", \"］\", \"【\", \"】\", \"‥\", \"〽\",\n",
    "          \"『\", \"』\", \"〝\", \"〟\", \"⟨\", \"⟩\", \"〜\", \"：\", \"！\", \"？\", \"♪\", \"؛\", \"/\", \"\\\\\", \"º\", \"−\", \"^\", \"'\", \"ʻ\", \"ˆ\"]\n",
    "chars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\"\n",
    "\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "kakasi = pykakasi.kakasi()\n",
    "kakasi.setMode(\"J\",\"H\")\n",
    "kakasi.setMode(\"K\",\"H\")\n",
    "kakasi.setMode(\"r\",\"Hepburn\")\n",
    "conv = kakasi.getConverter()\n",
    "\n",
    "def prepare_char(batch):\n",
    "    batch[\"transcription\"] = conv.do(wakati.parse(batch[\"transcription\"]).strip())\n",
    "    batch[\"transcription\"] = re.sub(chars_to_ignore_regex,'', batch[\"transcription\"]).strip()\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f1ad070-fed3-4699-bef6-12e8c1c5b6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f8e420e-95b2-413f-b7db-73d2465a5c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a48ad5852f42968e20eabf6a45b9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_997/970523411.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"transcription\"] = conv.do(wakati.parse(batch[\"transcription\"]).strip())\n",
      "/tmp/ipykernel_997/970523411.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"transcription\"] = conv.do(wakati.parse(batch[\"transcription\"]).strip())\n",
      "/tmp/ipykernel_997/970523411.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"transcription\"] = conv.do(wakati.parse(batch[\"transcription\"]).strip())\n",
      "/tmp/ipykernel_997/970523411.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"transcription\"] = conv.do(wakati.parse(batch[\"transcription\"]).strip())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f0899a212848e39025da810b06cd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 5270\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset = test_dataset.map(prepare_char, num_proc=4)\n",
    "prepare_eval_dataset = eval_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, num_proc=4)\n",
    "prepare_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "222027ee-43f3-4f28-8659-d76b211c819c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c472be4e2964550a4d387ea021f64de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 31.953978%\n",
      "CER: 16.177097%\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer = load(\"wer\")\n",
    "cer = load(\"cer\")\n",
    "\n",
    "def evaluate(batch):\n",
    "    pred_strings = []\n",
    "    for audio in batch[\"audio\"]:\n",
    "        inputs = processor(audio[\"array\"], sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs.input_values).logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        pred_ids[pred_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "        pred_string = processor.batch_decode(pred_ids)\n",
    "\n",
    "        pred_strings.extend(pred_string)\n",
    "\n",
    "    batch[\"pred_strings\"] = pred_strings\n",
    "    return batch\n",
    "\n",
    "columns_to_remove = [column for column in eval_dataset.column_names if column != \"transcription\"]\n",
    "result = eval_dataset.map(evaluate, remove_columns=columns_to_remove, batched=True)\n",
    "\n",
    "wer_result = wer.compute(predictions=result[\"pred_strings\"], references=result[\"transcription\"])\n",
    "cer_result = cer.compute(predictions=result[\"pred_strings\"], references=result[\"transcription\"])\n",
    "\n",
    "print(\"WER: {:2f}%\".format(100 * wer_result))\n",
    "print(\"CER: {:2f}%\".format(100 * cer_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791d4fc-e9c7-455f-8293-2a601d634cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
