{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9f484d-1344-489d-b58b-6f33c3ff8644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def make_score_table(file_path):\n",
    "    df_full = pd.read_excel(file_path)\n",
    "    df_reordered = df_full.set_index('Unnamed: 0').reset_index()\n",
    "    score_table = df_reordered.replace({'〇': float(1), 'O': float(1), '✖': np.nan, 'X': np.nan, '×': np.nan})\n",
    "    score_table.rename(columns={'Unnamed: 0': 'Text'}, inplace=True)\n",
    "    return score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235ed68c-9261-4708-839c-a6655177c88a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A_score_table = make_score_table(file_path='./語音辨識判定.xlsx')\n",
    "B_score_table = make_score_table(file_path='./判定のコピー.xlsx')\n",
    "C_score_table = make_score_table(file_path='./語音辨識判定(SHIH).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c99f4ed-0304-4ba4-a9b0-e647a2d68b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        audio_path   text  score\n",
      "0      ../../BLSTM/A_class/A_class_audio_1/わたし.mp3    わたし    1.0\n",
      "1    ../../BLSTM/A_class/A_class_audio_1/わたしたち.mp3  わたしたち    1.0\n",
      "2      ../../BLSTM/A_class/A_class_audio_1/あなた.mp3    あなた    1.0\n",
      "3     ../../BLSTM/A_class/A_class_audio_1/あのかた.mp3   あのかた    1.0\n",
      "4     ../../BLSTM/A_class/A_class_audio_1/みなさん.mp3   みなさん    1.0\n",
      "..                                             ...    ...    ...\n",
      "447     ../../BLSTM/C_class/C_class_audio_8/だれ.mp3     だれ    1.0\n",
      "448    ../../BLSTM/C_class/C_class_audio_8/どなた.mp3    どなた    1.0\n",
      "449    ../../BLSTM/C_class/C_class_audio_8/～さい.mp3    ～さい    1.0\n",
      "450   ../../BLSTM/C_class/C_class_audio_8/なんさい.mp3   なんさい    1.0\n",
      "451   ../../BLSTM/C_class/C_class_audio_8/おいくつ.mp3   おいくつ    1.0\n",
      "\n",
      "[452 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['audio_path', 'text', 'score'])\n",
    "\n",
    "rows = []\n",
    "\n",
    "for class_name, score_table in zip(['A', 'B', 'C'], [A_score_table, B_score_table, C_score_table]):\n",
    "    for person in range(10-2):\n",
    "        for index in range(23):\n",
    "            score = score_table.loc[index, f'音檔{person+1}']\n",
    "            text = score_table.loc[index, 'Text']\n",
    "            if pd.isna(score):\n",
    "                continue\n",
    "            audio_path = f'../../BLSTM/{class_name}_class/{class_name}_class_audio_{person+1}/{text}.mp3'\n",
    "            rows.append({'audio_path': audio_path, 'text': text, 'score': score})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d47318e-22c2-4345-abab-d2453ac54716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files_in_directory(directory):\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    file_list = []\n",
    "    \n",
    "    for file in files:\n",
    "        filepath = os.path.join(directory, file)\n",
    "        if os.path.isfile(filepath):\n",
    "            file_list.append(os.path.splitext(file)[0])\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d4199f6-3bee-4346-8e93-2a1fd9b5c96f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agmented_df = pd.DataFrame(columns=['audio_path', 'text', 'score'])\n",
    "\n",
    "rows = []\n",
    "\n",
    "for class_name, score_table in zip(['A', 'B', 'C'], [A_score_table, B_score_table, C_score_table]):\n",
    "    for person in range(10-2):\n",
    "        directory_path = f'./correct_augmented_audio/{class_name}_class/{class_name}_class_audio_{person+1}'\n",
    "        file_names = list_files_in_directory(directory_path)\n",
    "        for index in range(len(file_names)):\n",
    "            score = 0.0\n",
    "            text = file_names[index]\n",
    "            if pd.isna(score):\n",
    "                continue\n",
    "            audio_path = f'./correct_augmented_audio/{class_name}_class/{class_name}_class_audio_{person+1}/{text}.mp3'\n",
    "            rows.append({'audio_path': audio_path, 'text': text, 'score': score})\n",
    "\n",
    "augmented_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002bc123-af37-4b1f-a629-3b6ba3ad1f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c3ef36c-4d1e-424b-a005-0a560c1ce7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def process_waveforms(batch):\n",
    "\n",
    "    waveform, sample_rate = torchaudio.load(batch['audio_path'])\n",
    "\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # 如果 waveform 是雙聲道，需要轉單聲道。給 4GE用\n",
    "    if waveform.size(0) > 1:\n",
    "        waveform = waveform.mean(dim=0)\n",
    "\n",
    "    # 讓 waveform的維度正確\n",
    "    if waveform.ndim > 1:\n",
    "        waveform = waveform.squeeze()\n",
    "\n",
    "    batch[\"speech_array\"] = waveform\n",
    "    batch[\"sampling_rate\"] = 16000\n",
    "    \n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab257d4e-a1ee-4578-bc87-9c9b5d9e3823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4844ee93fd5441c94118e60006f2bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "audio_path = Dataset.from_pandas(combined_df)\n",
    "ds = audio_path.map(process_waveforms, remove_columns=['audio_path', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6029a772-69d7-4281-a9a0-07c38f2e74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "test_dataset = split_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d0c6f6-d254-4bee-84fe-01d9d6ab3cc0",
   "metadata": {},
   "source": [
    "# Trainer CTC process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2644fa6d-2ce7-4ff8-8a6a-0773e0348136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "from transformers import HubertForCTC\n",
    "import torch\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"TKU410410103/hubert-base-japanese-asr\")\n",
    "\n",
    "model = HubertForCTC.from_pretrained('TKU410410103/hubert-base-japanese-asr')\n",
    "# model = HubertForCTC.from_pretrained('./local_ASR/checkpoint-200_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "456e3ef8-bc20-438c-85e2-49edfe6a161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HubertForCTC(\n",
       "  (hubert): HubertModel(\n",
       "    (feature_extractor): HubertFeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): HubertGroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x HubertNoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x HubertNoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): HubertFeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): HubertEncoder(\n",
       "      (pos_conv_embed): HubertPositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): HubertSamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x HubertEncoderLayer(\n",
       "          (attention): HubertAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): HubertFeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=768, out_features=122, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa97d64e-5bb7-4cdc-a264-5bf1970ad941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9335/1922900375.py:14: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode(\"J\",\"H\")\n",
      "/tmp/ipykernel_9335/1922900375.py:15: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode(\"K\",\"H\")\n",
      "/tmp/ipykernel_9335/1922900375.py:16: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode(\"r\",\"Hepburn\")\n",
      "/tmp/ipykernel_9335/1922900375.py:17: DeprecationWarning: Call to deprecated method getConverter. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  conv = kakasi.getConverter()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import MeCab\n",
    "import pykakasi\n",
    "\n",
    "CHARS_TO_IGNORE = [\",\", \"?\", \"¿\", \".\", \"!\", \"¡\", \";\", \"；\", \":\", '\"\"', \"%\", '\"', \"�\", \"ʿ\", \"·\", \"჻\", \"~\", \"՞\",\n",
    "          \"؟\", \"،\", \"।\", \"॥\", \"«\", \"»\", \"„\", \"“\", \"”\", \"「\", \"」\", \"‘\", \"’\", \"《\", \"》\", \"(\", \")\", \"[\", \"]\",\n",
    "          \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"…\", \"–\", \"°\", \"´\", \"ʾ\", \"‹\", \"›\", \"©\", \"®\", \"—\", \"→\", \"。\",\n",
    "          \"、\", \"﹂\", \"﹁\", \"‧\", \"～\", \"﹏\", \"，\", \"｛\", \"｝\", \"（\", \"）\", \"［\", \"］\", \"【\", \"】\", \"‥\", \"〽\",\n",
    "          \"『\", \"』\", \"〝\", \"〟\", \"⟨\", \"⟩\", \"〜\", \"：\", \"！\", \"？\", \"♪\", \"؛\", \"/\", \"\\\\\", \"º\", \"−\", \"^\", \"'\", \"ʻ\", \"ˆ\"]\n",
    "chars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\"\n",
    "\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "kakasi = pykakasi.kakasi()\n",
    "kakasi.setMode(\"J\",\"H\")\n",
    "kakasi.setMode(\"K\",\"H\")\n",
    "kakasi.setMode(\"r\",\"Hepburn\")\n",
    "conv = kakasi.getConverter()\n",
    "\n",
    "def prepare_char(batch):\n",
    "    batch[\"sentence\"] = conv.do(wakati.parse(batch[\"text\"]).strip())\n",
    "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex,'', batch[\"text\"]).strip()\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3a2a67e-6595-4a6c-b403-42d5af4ec0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <enum '_TYPE'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/usr/local/lib/python3.10/dist-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <enum '_TYPE'>: pykakasi.kakasi._TYPE has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Parameter 'function'=<function prepare_char at 0x7f9a9a7c7250> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616de845337c4751b771d3e8d9bc7e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9335/1922900375.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"text\"]).strip())\n",
      "/tmp/ipykernel_9335/1922900375.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"text\"]).strip())\n",
      "/tmp/ipykernel_9335/1922900375.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"text\"]).strip())\n",
      "/tmp/ipykernel_9335/1922900375.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"text\"]).strip())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4e1d3ae6c24069bd78d5c9c5fc706c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/181 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9335/1922900375.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"text\"]).strip())\n",
      "/tmp/ipykernel_9335/1922900375.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"text\"]).strip())\n",
      "/tmp/ipykernel_9335/1922900375.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"text\"]).strip())\n",
      "/tmp/ipykernel_9335/1922900375.py:20: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"text\"]).strip())\n"
     ]
    }
   ],
   "source": [
    "encoded_train_dataset = train_dataset.map(prepare_char, num_proc=4)\n",
    "encoded_test_dataset = test_dataset.map(prepare_char, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86adb2d4-9876-4c70-bc99-89143430bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "\n",
    "    batch[\"input_values\"] = processor(batch[\"speech_array\"], sampling_rate=batch[\"sampling_rate\"]).input_values[0]\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7031997-164f-45ae-9e39-d2a21f3eb112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2202bb8022d0451297289d0a9308af66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da7ee86d87642e096a59f3e2dfa4170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/181 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_train_dataset = encoded_train_dataset.map(prepare_dataset, remove_columns=encoded_train_dataset.column_names, num_proc=4)\n",
    "encoded_test_dataset = encoded_test_dataset.map(prepare_dataset, remove_columns=encoded_test_dataset.column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "916787b8-ef8d-48d0-9910-96d5425c25cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1454: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_9335/261079650.py:48: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode(\"J\",\"H\")\n",
      "/tmp/ipykernel_9335/261079650.py:49: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode(\"K\",\"H\")\n",
      "/tmp/ipykernel_9335/261079650.py:50: DeprecationWarning: Call to deprecated method setMode. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  kakasi.setMode(\"r\",\"Hepburn\")\n",
      "/tmp/ipykernel_9335/261079650.py:51: DeprecationWarning: Call to deprecated method getConverter. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  conv = kakasi.getConverter()\n",
      "/usr/local/lib/python3.10/dist-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <enum '_TYPE'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/usr/local/lib/python3.10/dist-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <enum '_TYPE'>: pykakasi.kakasi._TYPE has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f4debc22d6403ea5167dfddfd7b73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9335/261079650.py:54: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"sentence\"]).strip())\n",
      "/tmp/ipykernel_9335/261079650.py:54: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"sentence\"]).strip())\n",
      "/tmp/ipykernel_9335/261079650.py:54: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"sentence\"]).strip())\n",
      "/tmp/ipykernel_9335/261079650.py:54: DeprecationWarning: Call to deprecated method do. (Old API will be removed in v3.0.) -- Deprecated since version 2.1.\n",
      "  batch[\"sentence\"] = conv.do(wakati.parse(batch[\"sentence\"]).strip())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7df0b1928ee4b188293a13797186756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('mozilla-foundation/common_voice_11_0', 'ja',split='train+validation')\n",
    "\n",
    "remove_columns = [col for col in train_dataset.column_names if col not in ['audio', 'sentence']]\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(remove_columns)\n",
    "# 隨機打亂數據集\n",
    "train_dataset = train_dataset.shuffle(seed=0)\n",
    "\n",
    "# 計算需要選取的數據量（10%）\n",
    "sample_size = int(0.2 * len(train_dataset))\n",
    "\n",
    "# 選取10%的數據\n",
    "train_dataset = train_dataset.select(range(sample_size))\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def process_waveforms(batch):\n",
    "    speech_arrays = []\n",
    "    sampling_rates = []\n",
    "\n",
    "    for audio_path in batch['audio']:\n",
    "        speech_array, _ = torchaudio.load(audio_path['path'])\n",
    "        speech_array_resampled = librosa.resample(np.asarray(speech_array[0].numpy()), orig_sr=48000, target_sr=16000)\n",
    "        speech_arrays.append(speech_array_resampled)\n",
    "        sampling_rates.append(16000)\n",
    "\n",
    "    batch[\"array\"] = speech_arrays\n",
    "    batch[\"sampling_rate\"] = sampling_rates\n",
    "\n",
    "    return batch\n",
    "resampled_train_dataset = train_dataset.map(process_waveforms, batched=True, batch_size=50, num_proc=4)\n",
    "import re\n",
    "import MeCab\n",
    "import pykakasi\n",
    "\n",
    "CHARS_TO_IGNORE = [\",\", \"?\", \"¿\", \".\", \"!\", \"¡\", \";\", \"；\", \":\", '\"\"', \"%\", '\"', \"�\", \"ʿ\", \"·\", \"჻\", \"~\", \"՞\",\n",
    "          \"؟\", \"،\", \"।\", \"॥\", \"«\", \"»\", \"„\", \"“\", \"”\", \"「\", \"」\", \"‘\", \"’\", \"《\", \"》\", \"(\", \")\", \"[\", \"]\",\n",
    "          \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"…\", \"–\", \"°\", \"´\", \"ʾ\", \"‹\", \"›\", \"©\", \"®\", \"—\", \"→\", \"。\",\n",
    "          \"、\", \"﹂\", \"﹁\", \"‧\", \"～\", \"﹏\", \"，\", \"｛\", \"｝\", \"（\", \"）\", \"［\", \"］\", \"【\", \"】\", \"‥\", \"〽\",\n",
    "          \"『\", \"』\", \"〝\", \"〟\", \"⟨\", \"⟩\", \"〜\", \"：\", \"！\", \"？\", \"♪\", \"؛\", \"/\", \"\\\\\", \"º\", \"−\", \"^\", \"'\", \"ʻ\", \"ˆ\"]\n",
    "chars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\"\n",
    "\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "kakasi = pykakasi.kakasi()\n",
    "kakasi.setMode(\"J\",\"H\")\n",
    "kakasi.setMode(\"K\",\"H\")\n",
    "kakasi.setMode(\"r\",\"Hepburn\")\n",
    "conv = kakasi.getConverter()\n",
    "\n",
    "def prepare_char(batch):\n",
    "    batch[\"sentence\"] = conv.do(wakati.parse(batch[\"sentence\"]).strip())\n",
    "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex,'', batch[\"sentence\"]).strip()\n",
    "    return batch\n",
    "a = resampled_train_dataset.map(prepare_char, num_proc=4)\n",
    "def prepare_dataset(batch):\n",
    "\n",
    "    batch[\"input_values\"] = processor(batch[\"array\"], sampling_rate=batch[\"sampling_rate\"]).input_values[0]\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "a = a.map(prepare_dataset, remove_columns=a.column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f923285-1a01-44f1-9575-d7aa304e712b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 2198\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9150b37d-edf4-4b97-a2c2-c33a79fe9521",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_values', 'labels'],\n",
       "    num_rows: 723\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d45b349e-8688-4e05-a5ba-8b59012e175e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_values', 'labels'],\n",
      "    num_rows: 2921\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "# 合併數據集\n",
    "combined_dataset = concatenate_datasets([a, encoded_train_dataset])\n",
    "\n",
    "# 檢查合併後的數據集\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eca0bf6f-43ae-458d-b548-196813bb557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True  # Ensures padding is enabled\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad input features to ensure uniform length\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "          )\n",
    "\n",
    "        # Process and pad labels separately to ensure uniform length\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "              label_features,\n",
    "              padding=self.padding,\n",
    "              max_length=self.max_length_labels,\n",
    "              pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "              return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "    # Mask padding in labels to ignore them in loss calculation\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch[\"attention_mask\"].ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ddeaa568-ee40-469b-b280-83fa5f2a48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # print(f'pred: {pred}')\n",
    "\n",
    "    pred_logits = pred.predictions\n",
    "    # print(f'logits: {pred_logits}')\n",
    "\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    # print(f'pred_ids: {pred_ids}')\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    # print(f'label_ids: {pred.label_ids}')\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    # print(f'pred_str: {pred_str}')\n",
    "    # print(f'label_str: {label_str}')\n",
    "\n",
    "    wer_result = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    " \n",
    "    return {\"wer\": wer_result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28e9155b-47cf-4181-9b68-33013b6cdb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, lr_end=5e-10, power=1.2, last_epoch=-1):\n",
    "\n",
    "    lr_init = optimizer.defaults[\"lr\"]\n",
    "    assert lr_init > lr_end, f\"lr_end ({lr_end}) must be be smaller than initial lr ({lr_init})\"\n",
    "\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        elif current_step > num_training_steps:\n",
    "            return lr_end / lr_init\n",
    "        else:\n",
    "            lr_range = lr_init - lr_end\n",
    "            decay_steps = num_training_steps - num_warmup_steps\n",
    "            pct_remaining = 1 - (current_step - num_warmup_steps) / decay_steps\n",
    "            decay = lr_range * pct_remaining ** power + lr_end\n",
    "            return decay / lr_init\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "class PolyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def create_scheduler(self, num_training_steps: int):\n",
    "        self.lr_scheduler = get_polynomial_decay_schedule_with_warmup(\n",
    "                self.optimizer,\n",
    "                num_warmup_steps=self.args.warmup_steps,\n",
    "                num_training_steps=num_training_steps\n",
    "                )\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        self.create_optimizer()\n",
    "        self.create_scheduler(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "598aa433-46c4-4de1-83b7-4257d3cc520f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6ab88bb-1f80-4ea2-acdf-119a1edba67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 03:50, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.209798</td>\n",
       "      <td>0.209945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.188900</td>\n",
       "      <td>0.215470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.613100</td>\n",
       "      <td>0.210863</td>\n",
       "      <td>0.220994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.607300</td>\n",
       "      <td>0.209725</td>\n",
       "      <td>0.220994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py:156: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Could not locate the best model at ./local_ASR/checkpoint-100/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.6144875907897949, metrics={'train_runtime': 231.9725, 'train_samples_per_second': 55.179, 'train_steps_per_second': 1.724, 'total_flos': 3.263808125649112e+17, 'train_loss': 0.6144875907897949, 'epoch': 4.37})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./local_ASR\",\n",
    "  per_device_train_batch_size=16, # 調小以適應 CPU\n",
    "  gradient_accumulation_steps=2,\n",
    "  learning_rate=1e-5,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=30,\n",
    "  # num_train_epochs=6,\n",
    "  max_steps=400,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True, # False => 不用 GPU\n",
    "  group_by_length=True,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  per_device_eval_batch_size=16, # 調小以適應 CPU\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=100,\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"wer\",\n",
    "  greater_is_better=False,\n",
    "  push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = PolyTrainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=combined_dataset,\n",
    "  eval_dataset=encoded_test_dataset,\n",
    "  tokenizer=processor.feature_extractor, # 原本填 processor\n",
    "  data_collator=data_collator,\n",
    "  compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d84bfd-39eb-470c-970a-8489d3aa1f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
